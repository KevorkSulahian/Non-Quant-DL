{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries required\n",
    "\n",
    "azure-ai-ml\n",
    "\n",
    "azure-identity\n",
    "\n",
    "datasets\n",
    "\n",
    "mlflow\n",
    "\n",
    "azureml-mlflow\n",
    "\n",
    "py7zr\n",
    "\n",
    "ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wether you're running on azure notebook or a local one we first need to connect to an azure instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    ")\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "import time\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "try:\n",
    "    workspace_ml_client = MLClient.from_config(credential=credential)\n",
    "except:\n",
    "    workspace_ml_client = MLClient(\n",
    "        credential,\n",
    "        subscription_id=\"<SUBSCRIPTION_ID>\",\n",
    "        resource_group_name=\"<RESOURCE_GROUP>\",\n",
    "        workspace_name=\"<WORKSPACE_NAME>\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the models, fine tuning pipelines and environments are available in the AzureML system registry, \"azureml\"\n",
    "\n",
    "everything that we will be working with are available in the registry of azureml.\n",
    "Check different registeries to see which model is available for example if we want to work with or finetune a llama model we can find them in the meta registery azureml-meta\n",
    "\n",
    "The pipelines and eviroments are also available in the registery too\n",
    "\n",
    "(insert image of ui maybe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. registry_name: This is the name of the model registry where the model is stored. In this case, it’s set to “azureml-meta”, which is a public registry that contains Llama 2 models.\n",
    "\n",
    "2. model_name: This is the name of the model to be deployed. Here, it’s set to “Llama-2–7b”. There are other available models for text generation.\n",
    "\n",
    "3. endpoint_name: This is the name of the endpoint where the model will be deployed. It’s set to the model name followed by “-test-ep”. You should replace this with your own endpoint name.\n",
    "\n",
    "4. deployment_name: This is the name of the deployment. It’s be default set to “llama”, but you should replace this with your own deployment name. Note that it should be in lowercase only.\n",
    "\n",
    "5. sku_name: This is the name of the SKU (stock keeping unit), which represents the instance type for the deployment. It’s set to “Standard_NC24s_v3”, but you should check the model list to find the most optimal SKU for your model.\n",
    "\n",
    "6. content_severity_threshold: This is the severity level that will trigger the response to be blocked. It’s set to “2”. For more details, you can refer to the Azure AI content documentation.\n",
    "\n",
    "7. uai_name: This is the name of the User-Assigned Identity (UAI) to be used for endpoint authentication. It’s currently empty, but defaults to “aacs-uai” in the prepare UAI notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_ml_client = MLClient(credential, registry_name=\"azureml\")\n",
    "registry_ml_client_meta = MLClient(credential, registry_name=\"azureml-meta\")\n",
    "experiment_name = \"text-generation\"\n",
    "\n",
    "# generating a unique timestamp that can be used for names and versions that need to be unique\n",
    "timestamp = str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"Llama-2-70b\"\n",
    "\n",
    "foundation_model = registry_ml_client_meta.models.get(model_name, label=\"latest\")\n",
    "print(\n",
    "    \"\\n\\nUsing model name: {0}, version: {1}, id: {2} for fine tuning\".format(\n",
    "        foundation_model.name, foundation_model.version, foundation_model.id\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU cluster\n",
    "\n",
    "if \"computes_allow_list\" in foundation_model.tags:\n",
    "    computes_allow_list = ast.literal_eval(\n",
    "        foundation_model.tags[\"computes_allow_list\"]\n",
    "    )  # convert string to python list\n",
    "    computes_allow_list.append(\"Standard_NC48ads_A100_v4\")\n",
    "    computes_allow_list.append(\"Standard_NC96ads_A100_v4\") \n",
    "    print(f\"Please create a compute from the above list - {computes_allow_list}\")\n",
    "else:\n",
    "    computes_allow_list = None\n",
    "    print(\"Computes allow list is not part of model tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a specific compute size to work with change it here. By default we use the 8 x V100 compute from the above list\n",
    "compute_cluster_size = \"Standard_NC96ads_A100_v4\"\n",
    "\n",
    "# If you already have a gpu cluster, mention it here. Else will create a new one with the name 'gpu-cluster-big'\n",
    "#compute_cluster = \"gpu-cluster-big\"\n",
    "compute_cluster = \"gpu-cluster-large2\"\n",
    "\n",
    "try:\n",
    "    compute = workspace_ml_client.compute.get(compute_cluster)\n",
    "    print(\"The compute cluster already exists! Reusing it for the current run\")\n",
    "except Exception as ex:\n",
    "    print(\n",
    "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {compute_cluster_size}!\"\n",
    "    )\n",
    "    try:\n",
    "        print(\"Attempt #1 - Trying to create a dedicated compute\")\n",
    "        compute = AmlCompute(\n",
    "            name=compute_cluster,\n",
    "            size=compute_cluster_size,\n",
    "            tier=\"Dedicated\",\n",
    "            max_instances=2,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            print(\n",
    "                \"Attempt #2 - Trying to create a low priority compute. Since this is a low priority compute, the job could get pre-empted before completion.\"\n",
    "            )\n",
    "            compute = AmlCompute(\n",
    "                name=compute_cluster,\n",
    "                size=compute_cluster_size,\n",
    "                tier=\"LowPriority\",\n",
    "                max_instances=2,  # For multi node training set this to an integer value more than 1\n",
    "            )\n",
    "            workspace_ml_client.compute.begin_create_or_update(compute).wait()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise ValueError(\n",
    "                f\"WARNING! Compute size {compute_cluster_size} not available in workspace\"\n",
    "            )\n",
    "\n",
    "\n",
    "# Wait for the compute to be created\n",
    "\n",
    "\n",
    "# This is the number of GPUs in a single node of the selected 'vm_size' compute.\n",
    "# Setting this to less than the number of GPUs will result in underutilized GPUs, taking longer to train.\n",
    "# Setting this to more than the number of GPUs will result in an error.\n",
    "gpu_count_found = False\n",
    "workspace_compute_sku_list = workspace_ml_client.compute.list_sizes()\n",
    "available_sku_sizes = []\n",
    "for compute_sku in workspace_compute_sku_list:\n",
    "    available_sku_sizes.append(compute_sku.name)\n",
    "    if compute_sku.name.lower() == compute.size.lower():\n",
    "        gpus_per_node = compute_sku.gpus\n",
    "        gpu_count_found = True\n",
    "        print(compute_sku.name.lower())\n",
    "# if gpu_count_found not found, then print an error\n",
    "if gpu_count_found:\n",
    "    print(f\"Number of GPU's in compute {compute.size}: {gpus_per_node}\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Number of GPU's in compute {compute.size} not found. Available skus are: {available_sku_sizes}.\"\n",
    "        f\"This should not happen. Please check the selected compute cluster: {compute_cluster} and try again.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the model being used it can either accept text or a tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the finetune data from SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed(df):\n",
    "    prompt = f\"Summarize this dialog:\\n{{}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "    df[\"text\"] = df[\"dialogue\"].map(prompt.format)\n",
    "    df = df.drop(columns=[\"dialogue\", \"id\"])\n",
    "    df = df[[\"text\", \"summary\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the cluster of gpu's and model used please make sure the change the per_device_train_batch_size and per_device_eval_batch_size.\n",
    "\n",
    "For example a 7b model with 4 A100 gpu's could have a maximum batch size of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "training_parameters = dict(\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "print(f\"The following training parameters are enabled - {training_parameters}\")\n",
    "\n",
    "# Optimization parameters - As these parameters are packaged with the model itself, lets retrieve those parameters\n",
    "if \"model_specific_defaults\" in foundation_model.tags:\n",
    "    optimization_parameters = ast.literal_eval(\n",
    "        foundation_model.tags[\"model_specific_defaults\"]\n",
    "    )  # convert string to python dict\n",
    "else:\n",
    "    optimization_parameters = dict(\n",
    "        apply_lora=\"true\", apply_deepspeed=\"true\", apply_ort=\"true\"\n",
    "    )\n",
    "print(f\"The following optimizations are enabled - {optimization_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import CommandComponent, PipelineComponent, Job, Component\n",
    "from azure.ai.ml import PyTorchDistribution, Input\n",
    "\n",
    "# fetch the pipeline component\n",
    "pipeline_component_func = registry_ml_client.components.get(\n",
    "    name=\"text_generation_pipeline\", label=\"latest\"\n",
    ")\n",
    "\n",
    "\n",
    "# define the pipeline job\n",
    "@pipeline()\n",
    "def create_pipeline():\n",
    "    text_generation_pipeline = pipeline_component_func(\n",
    "\n",
    "\n",
    "        ####### Depending on the model you are using, you can use either the model you have or found on huggingface or the model's found in the registery\n",
    "\n",
    "        # mlflow_model_path=foundation_model.id, # Uncomment this line if you want to use a model from the registry and comment the line below\n",
    "        # huggingface_id = 'meta-llama/Llama-2-7b', # if you want to use a huggingface model, uncomment this line and comment the above line\n",
    "\n",
    "        ########\n",
    "        compute_model_import=compute_cluster,\n",
    "        compute_preprocess=compute_cluster,\n",
    "        compute_finetune=compute_cluster,\n",
    "        compute_model_evaluation=compute_cluster,\n",
    "        # map the dataset splits to parameters\n",
    "        train_file_path=Input(\n",
    "            type=\"uri_file\", path=\"<path to train>\"\n",
    "        ),\n",
    "        validation_file_path=Input(\n",
    "            type=\"uri_file\", path=\"<path to validation>\"\n",
    "        ),\n",
    "        test_file_path=Input(type=\"uri_file\", path=\"<path to test>\"),\n",
    "        evaluation_config=Input(type=\"uri_file\", path=\"<path to evaluation_config>\"),\n",
    "        # map the preprocessed data to parameters\n",
    "        # The following parameters map to the dataset fields\n",
    "        text_key=\"text\",\n",
    "        ground_truth_key=\"summary\",\n",
    "        num_nodes_finetune=2,\n",
    "        # Training settings\n",
    "        number_of_gpu_to_use_finetuning=gpus_per_node,  # set to the number of GPUs available in the compute\n",
    "        **training_parameters,\n",
    "        **optimization_parameters\n",
    "    )\n",
    "    return {\n",
    "        # map the output of the fine tuning job to the output of pipeline job so that we can easily register the fine tuned model\n",
    "        # registering the model is required to deploy the model to an online or batch endpoint\n",
    "        \"trained_model\": text_generation_pipeline.outputs.mlflow_model_folder\n",
    "    }\n",
    "\n",
    "\n",
    "pipeline_object = create_pipeline()\n",
    "\n",
    "# don't use cached results from previous jobs\n",
    "pipeline_object.settings.force_rerun = True\n",
    "\n",
    "# set continue on step failure to False\n",
    "pipeline_object.settings.continue_on_step_failure = False\n",
    "\n",
    "# set the pytorch and mlflow mode to mount\n",
    "\n",
    "pipeline_object.jobs[\"text_generation_pipeline\"][\"outputs\"][\"pytorch_model_folder\"].mode = \"mount\"\n",
    "\n",
    "pipeline_object.jobs[\"text_generation_pipeline\"][\"outputs\"][\"mlflow_model_folder\"].mode = \"mount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = workspace_ml_client.jobs.create_or_update(\n",
    "    pipeline_object, experiment_name=experiment_name\n",
    ")\n",
    "# wait for the pipeline job to complete\n",
    "workspace_ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the model (some question here)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## this part was copied from the documentation,not sure if it will work\n",
    "\n",
    "# from azure.ai.ml.entities import (\n",
    "#     OnlineRequestSettings,\n",
    "#     CodeConfiguration,\n",
    "#     ManagedOnlineDeployment,\n",
    "#     ProbeSettings,\n",
    "# )\n",
    "\n",
    "# deployment = ManagedOnlineDeployment(\n",
    "#     name=deployment_name,\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     model=llama_model.id,\n",
    "#     instance_type=sku_name,\n",
    "#     instance_count=1,\n",
    "#     environment = \"llama2-environment:1\"\n",
    "#     environment_variables=deployment_env_vars,\n",
    "#     request_settings=OnlineRequestSettings(request_timeout_ms=REQUEST_TIMEOUT_MS),\n",
    "#     liveness_probe=ProbeSettings(\n",
    "#         failure_threshold=30,\n",
    "#         success_threshold=1,\n",
    "#         period=100,\n",
    "#         initial_delay=500,\n",
    "#     ),\n",
    "#     readiness_probe=ProbeSettings(\n",
    "#         failure_threshold=30,\n",
    "#         success_threshold=1,\n",
    "#         period=100,\n",
    "#         initial_delay=500,\n",
    "#     ),\n",
    "# )\n",
    "# # Trigger the deployment creation\n",
    "# try:\n",
    "#     ml_client.begin_create_or_update(deployment).wait()\n",
    "#     print(\"\\n---Deployment created successfully---\\n\")\n",
    "# except Exception as err:\n",
    "#     raise RuntimeError(\n",
    "#         f\"Deployment creation failed. Detailed Response:\\n{err}\"\n",
    "#     ) from err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# test_src_dir = \"./llama-test\"\n",
    "# os.makedirs(test_src_dir, exist_ok=True)\n",
    "# print(f\"test script directory: {test_src_dir}\")\n",
    "# sample_data = os.path.join(test_src_dir, \"sample-request.json\")\n",
    "\n",
    "# ## For text-generation models (without -chat suffix)\n",
    "# ## Successful response\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open(sample_data, \"w\") as f:\n",
    "#     json.dump(\n",
    "#         {\n",
    "#             \"input_data\": {\n",
    "#                 \"input_string\": [\n",
    "#                     \"Hello\",\n",
    "#                     \"My name is John and I have a dog.\",\n",
    "#                 ],\n",
    "#                 \"parameters\": {\n",
    "#                     \"temperature\": 0.6,\n",
    "#                     \"top_p\": 0.6,\n",
    "#                     \"max_new_tokens\": 256,\n",
    "#                     \"do_sample\": True,\n",
    "#                 },\n",
    "#             }\n",
    "#         },\n",
    "#         f,\n",
    "#     )\n",
    "\n",
    "# ml_client.online_endpoints.invoke(\n",
    "#     endpoint_name=endpoint_name,\n",
    "#     deployment_name=deployment_name,\n",
    "#     request_file=sample_data,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
