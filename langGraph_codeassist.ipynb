{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 0.0.10 Requires-Python >=3.9.0,<4.0; 0.0.11 Requires-Python >=3.9.0,<4.0; 0.0.12 Requires-Python >=3.9.0,<4.0; 0.0.13 Requires-Python >=3.9.0,<4.0; 0.0.14 Requires-Python >=3.9.0,<4.0; 0.0.15 Requires-Python >=3.9.0,<4.0; 0.0.16 Requires-Python >=3.9.0,<4.0; 0.0.17 Requires-Python >=3.9.0,<4.0; 0.0.18 Requires-Python >=3.9.0,<4.0; 0.0.19 Requires-Python >=3.9.0,<4.0; 0.0.20 Requires-Python >=3.9.0,<4.0; 0.0.21 Requires-Python >=3.9.0,<4.0; 0.0.22 Requires-Python >=3.9.0,<4.0; 0.0.23 Requires-Python >=3.9.0,<4.0; 0.0.24 Requires-Python >=3.9.0,<4.0; 0.0.25 Requires-Python >=3.9.0,<4.0; 0.0.26 Requires-Python >=3.9.0,<4.0; 0.0.27 Requires-Python >=3.9.0,<4.0; 0.0.28 Requires-Python >=3.9.0,<4.0; 0.0.9 Requires-Python >=3.9.0,<4.0; 0.1.1 Requires-Python >=3.9\n",
      "ERROR: Could not find a version that satisfies the requirement langgraph (from versions: 0.0.8)\n",
      "ERROR: No matching distribution found for langgraph\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# LCEL w/ PydanticOutputParser (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_pydantic = loader.load()\n",
    "\n",
    "# LCEL w/ Self Query (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_sq = loader.load()\n",
    "\n",
    "# Add\n",
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "\n",
    "# Sort the list based on the URLs in 'metadata' -> 'source'\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "\n",
    "# Concatenate the 'page_content' of each sorted dictionary\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution based on LCEL docs and the input question\n",
    "    with optional feedback from code execution tests\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "\n",
    "    ## State\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    iter = state_dict[\"iterations\"]\n",
    "\n",
    "    ## Data model\n",
    "    class code(BaseModel):\n",
    "        \"\"\"Code output\"\"\"\n",
    "\n",
    "        prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "        imports: str = Field(description=\"Code block import statements\")\n",
    "        code: str = Field(description=\"Code block not including import statements\")\n",
    "\n",
    "    ## LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\n",
    "    # Tool\n",
    "    code_tool_oai = convert_to_openai_tool(code)\n",
    "\n",
    "    # LLM with tool and enforce invocation\n",
    "    llm_with_tool = model.bind(\n",
    "        tools=[code_tool_oai],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"code\"}},\n",
    "    )\n",
    "\n",
    "    # Parser\n",
    "    parser_tool = PydanticToolsParser(tools=[code])\n",
    "\n",
    "    ## Prompt\n",
    "    template = \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "        Here is a full set of LCEL documentation: \n",
    "        \\n ------- \\n\n",
    "        {context} \n",
    "        \\n ------- \\n\n",
    "        Answer the user question based on the above provided documentation. \\n\n",
    "        Ensure any code you provide can be executed with all required imports and variables defined. \\n\n",
    "        Structure your answer with a description of the code solution. \\n\n",
    "        Then list the imports. And finally list the functioning code block. \\n\n",
    "        Here is the user question: \\n --- --- --- \\n {question}\"\"\"\n",
    "\n",
    "    ## Generation\n",
    "    if \"error\" in state_dict:\n",
    "        print(\"---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\")\n",
    "\n",
    "        error = state_dict[\"error\"]\n",
    "        code_solution = state_dict[\"generation\"]\n",
    "\n",
    "        # Udpate prompt\n",
    "        addendum = \"\"\"  \\n --- --- --- \\n You previously tried to solve this problem. \\n Here is your solution:  \n",
    "                    \\n --- --- --- \\n {generation}  \\n --- --- --- \\n  Here is the resulting error from code \n",
    "                    execution:  \\n --- --- --- \\n {error}  \\n --- --- --- \\n Please re-try to answer this. \n",
    "                    Structure your answer with a description of the code solution. \\n Then list the imports. \n",
    "                    And finally list the functioning code block. Structure your answer with a description of \n",
    "                    the code solution. \\n Then list the imports. And finally list the functioning code block. \n",
    "                    \\n Here is the user question: \\n --- --- --- \\n {question}\"\"\"\n",
    "        template = template + addendum\n",
    "\n",
    "        # Prompt\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\", \"generation\", \"error\"],\n",
    "        )\n",
    "\n",
    "        # Chain\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": lambda _: concatenated_content,\n",
    "                \"question\": itemgetter(\"question\"),\n",
    "                \"generation\": itemgetter(\"generation\"),\n",
    "                \"error\": itemgetter(\"error\"),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm_with_tool\n",
    "            | parser_tool\n",
    "        )\n",
    "\n",
    "        code_solution = chain.invoke(\n",
    "            {\"question\": question, \"generation\": str(code_solution[0]), \"error\": error}\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"---GENERATE SOLUTION---\")\n",
    "\n",
    "        # Prompt\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "        )\n",
    "\n",
    "        # Chain\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": lambda _: concatenated_content,\n",
    "                \"question\": itemgetter(\"question\"),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm_with_tool\n",
    "            | parser_tool\n",
    "        )\n",
    "\n",
    "        code_solution = chain.invoke({\"question\": question})\n",
    "\n",
    "    iter = iter + 1\n",
    "    return {\n",
    "        \"keys\": {\"generation\": code_solution, \"question\": question, \"iterations\": iter}\n",
    "    }\n",
    "\n",
    "\n",
    "def check_code_imports(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check imports\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    ## State\n",
    "    print(\"---CHECKING CODE IMPORTS---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    code_solution = state_dict[\"generation\"]\n",
    "    imports = code_solution[0].imports\n",
    "    iter = state_dict[\"iterations\"]\n",
    "\n",
    "    try:\n",
    "        # Attempt to execute the imports\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        # Catch any error during execution (e.g., ImportError, SyntaxError)\n",
    "        error = f\"Execution error: {e}\"\n",
    "        if \"error\" in state_dict:\n",
    "            error_prev_runs = state_dict[\"error\"]\n",
    "            error = error_prev_runs + \"\\n --- Most recent run error --- \\n\" + error\n",
    "    else:\n",
    "        print(\"---CODE IMPORT CHECK: SUCCESS---\")\n",
    "        # No errors occurred\n",
    "        error = \"None\"\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"generation\": code_solution,\n",
    "            \"question\": question,\n",
    "            \"error\": error,\n",
    "            \"iterations\": iter,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def check_code_execution(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check code block execution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    ## State\n",
    "    print(\"---CHECKING CODE EXECUTION---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    code_solution = state_dict[\"generation\"]\n",
    "    prefix = code_solution[0].prefix\n",
    "    imports = code_solution[0].imports\n",
    "    code = code_solution[0].code\n",
    "    code_block = imports + \"\\n\" + code\n",
    "    iter = state_dict[\"iterations\"]\n",
    "\n",
    "    try:\n",
    "        # Attempt to execute the code block\n",
    "        exec(code_block)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "        # Catch any error during execution (e.g., ImportError, SyntaxError)\n",
    "        error = f\"Execution error: {e}\"\n",
    "        if \"error\" in state_dict:\n",
    "            error_prev_runs = state_dict[\"error\"]\n",
    "            error = error_prev_runs + \"\\n --- Most recent run error --- \\n\" + error\n",
    "    else:\n",
    "        print(\"---CODE BLOCK CHECK: SUCCESS---\")\n",
    "        # No errors occurred\n",
    "        error = \"None\"\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"generation\": code_solution,\n",
    "            \"question\": question,\n",
    "            \"error\": error,\n",
    "            \"prefix\": prefix,\n",
    "            \"imports\": imports,\n",
    "            \"iterations\": iter,\n",
    "            \"code\": code,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_check_code_exec(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to test code execution, or re-try answer generation.\n",
    "\n",
    "    Args:\n",
    "       state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO TEST CODE EXECUTION---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    error = state_dict[\"error\"]\n",
    "\n",
    "    if error == \"None\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: TEST CODE EXECUTION---\")\n",
    "        return \"check_code_execution\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish (re-try code 3 times.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO TEST CODE EXECUTION---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    error = state_dict[\"error\"]\n",
    "    iter = state_dict[\"iterations\"]\n",
    "\n",
    "    if error == \"None\" or iter == 3:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: TEST CODE EXECUTION---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"generate\", generate)  # generation solution\n",
    "workflow.add_node(\"check_code_imports\", check_code_imports)  # check imports\n",
    "workflow.add_node(\"check_code_execution\", check_code_execution)  # check execution\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"generate\")\n",
    "workflow.add_edge(\"generate\", \"check_code_imports\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_code_imports\",\n",
    "    decide_to_check_code_exec,\n",
    "    {\n",
    "        \"check_code_execution\": \"check_code_execution\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_code_execution\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith\n",
    "\n",
    "client = langsmith.Client()\n",
    "\n",
    "public_dataset = (\n",
    "    \"https://smith.langchain.com/public/326674a6-62bd-462d-88ae-eea49d503f9d/d\"\n",
    ")\n",
    "# Clone the dataset to your tenant to use it\n",
    "client.clone_public_dataset(public_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "## Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Code output\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "\n",
    "\n",
    "## LLM\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\n",
    "# Tool\n",
    "code_tool_oai = convert_to_openai_tool(code)\n",
    "\n",
    "# LLM with tool and enforce invocation\n",
    "llm_with_tool = model.bind(\n",
    "    tools=[convert_to_openai_tool(code_tool_oai)],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"code\"}},\n",
    ")\n",
    "\n",
    "# Parser\n",
    "parser_tool = PydanticToolsParser(tools=[code])\n",
    "\n",
    "# Create a prompt template with format instructions and the query\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "        Here is a full set of LCEL documentation: \n",
    "        \\n ------- \\n\n",
    "        {context} \n",
    "        \\n ------- \\n\n",
    "        Answer the user question based on the above provided documentation. \\n\n",
    "        Ensure any code you provide can be executed with all required imports and variables defined. \\n\n",
    "        Structure your answer with a description of the code solution. \\n\n",
    "        Then list the imports. And finally list the functioning code block. \\n\n",
    "        Here is the user question: \\n --- --- --- \\n {question}\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "\n",
    "def parse_answer_to_dict(x):\n",
    "    return x[0].dict()\n",
    "\n",
    "\n",
    "chain_base_case = (\n",
    "    {\n",
    "        \"context\": lambda _: concatenated_content,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tool\n",
    "    | parser_tool\n",
    "    | RunnableLambda(parse_answer_to_dict)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prefix': \"To write a Retrieval-Augmented Generation (RAG) chain using LangChain Expression Language (LCEL), you can follow the steps outlined below. This example demonstrates how to create a RAG chain that incorporates a retrieval step to fetch relevant context before generating a response to a user's query. The chain consists of a retriever component to fetch relevant documents, a prompt template to format the query and context for the language model, and a language model to generate the response.\\n\\n\",\n",
       " 'imports': 'from operator import itemgetter\\nfrom langchain_community.vectorstores import FAISS\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.runnables import RunnablePassthrough\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings',\n",
       " 'code': '# Initialize the vector store with sample texts and embeddings\\nvectorstore = FAISS.from_texts(\\n    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())\\n\\n# Create a retriever from the vector store\\nretriever = vectorstore.as_retriever()\\n\\n# Define the prompt template\\ntemplate = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"\\nprompt = ChatPromptTemplate.from_template(template)\\n\\n# Initialize the language model\\nmodel = ChatOpenAI()\\n\\n# Define the RAG chain\\nchain = (\\n    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser()\\n)\\n\\n# Invoke the chain with a user query\\nresponse = chain.invoke(\"where did harrison work?\")\\nprint(response)'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = chain_base_case.invoke(\"How can I write a RAG chain?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### No LangGraph\n",
    "\n",
    "import uuid\n",
    "from typing import Union\n",
    "\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import EvaluationResult\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "\n",
    "def check_import(run: Run, example: Union[Example, None] = None):\n",
    "    model_outputs = run.outputs\n",
    "    imports = model_outputs[\"imports\"]\n",
    "    try:\n",
    "        exec(imports)\n",
    "        score = 1\n",
    "    except:\n",
    "        score = 0\n",
    "    return EvaluationResult(key=\"check_import\", score=score)\n",
    "\n",
    "\n",
    "def check_execution(run: Run, example: Union[Example, None] = None):\n",
    "    model_outputs = run.outputs\n",
    "    imports = model_outputs[\"imports\"]\n",
    "    code = model_outputs[\"code\"]\n",
    "    code_to_execute = imports + \"\\n\" + code\n",
    "    try:\n",
    "        exec(code_to_execute)\n",
    "        score = 1\n",
    "    except:\n",
    "        score = 0\n",
    "    return EvaluationResult(key=\"check_execution\", score=score)\n",
    "\n",
    "\n",
    "# Config\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[check_import, check_execution],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '2ec7-context-stuffing-no-langgraph' at:\n",
      "https://smith.langchain.com/o/251ecdd9-2ca8-5dd9-ab04-c02e1fdaa47b/datasets/35eb6da5-2be1-4d79-b992-c8c8032be8ce/compare?selectedSessions=53708011-3fa4-4480-97a9-ed998714a35b\n",
      "\n",
      "View all tests for Dataset lcel-teacher-eval at:\n",
      "https://smith.langchain.com/o/251ecdd9-2ca8-5dd9-ab04-c02e1fdaa47b/datasets/35eb6da5-2be1-4d79-b992-c8c8032be8ce\n",
      "[>                                                 ] 0/20content=\"Why don't bears wear socks?\\n\\nBecause they have bear feet!\" response_metadata={'finish_reason': 'stop', 'logprobs': None}\n",
      "[->                                                ] 1/20{'joke': \"Why don't bears like fast food?\\n\\nBecause they can't catch it!\", 'poem': \"In the heart of the forest, wild and free\\nRoams a creature, strong and mighty\\nWith fur as dark as the night\\nAnd eyes that gleam with pure delight\\n\\nThe bear, a symbol of power and grace\\nNavigating the woods with steady pace\\nA gentle giant, yet fierce when provoked\\nIn its presence, all are awed and cloaked\\n\\nFrom the smallest cub to the largest grizzly\\nThese creatures are both feared and frizzly\\nBut in their world, they reign supreme\\nKings of the forest, a majestic dream\\n\\nSo let us admire these creatures of might\\nAnd cherish their presence, day and night\\nFor the bear is a symbol of nature's grace\\nA reminder of the wild in this human race.\"}\n",
      "[--------->                                        ] 4/20"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 9fed0144-cd6b-4c61-918e-b88fd226167a with inputs {'question': 'How can I use a custom function to route between 2 chains in LCEL?'}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 289437, Requested 74889. Please try again in 12.865s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----------->                                      ] 5/20{'title': 'PromptInput', 'type': 'object', 'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}\n",
      "[----------------->                                ] 7/20Why did the bear break up with his girlfriend? \n",
      "\n",
      "Because he couldn't bear the relationship any longer!\n",
      "{'history': [HumanMessage(content='hi im bob'), AIMessage(content='Hello Bob! How can I assist you today?')]}\n",
      "[--------------------->                            ] 9/20"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 37b94cd3-d58a-4b9b-bbc1-09d3d8f8fec9 with inputs {'question': \"My LCEL map contains the key 'question'. What is the difference between using itemgetter('question'), lambda x: x['question'], and x.get('question')?\"}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 283927, Requested 74909. Please try again in 11.767s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------>                         ] 10/20"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example bb3c615b-4fd2-4f62-84ab-ec0da8982a05 with inputs {'question': \"I have a LCEL runnable, chain, and am passing in a map w/ {'question' 'where did harrison work', 'language': 'italian'}. How can I extract the value of 'language' to pass to my prompt?\"}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 295550, Requested 74918. Please try again in 14.093s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----------------------------->                    ] 12/20"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 577aa8c7-cbe7-49fe-bfa2-c3ceb48fbb68 with inputs {'question': \"I'm invoking a LCEL chain with a map that contain {'question': 'how do I use Anthropic?'}. The full chain definition is full_chain = {'question': lambda x: x['question']} | sub_chain. Why is a lambda used?\"}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 291978, Requested 74923. Please try again in 13.38s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------------------------->               ] 14/20"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example ceefc5fe-b782-46a7-9bf5-d95d0d7fcd40 with inputs {'question': \"I am passing a map with {'num': 1} to a LCEL chain. How can I add an extra key num2 to this map that adds 1 to the value of num. Then I want to assign this new map to a new key named output?\"}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 286975, Requested 74920. Please try again in 12.379s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------->            ] 15/20\n",
      "[----------------------------------------->        ] 17/20content='Why do bears have hairy coats?\\n\\nFur protection!' response_metadata={'finish_reason': 'stop', 'logprobs': None}\n",
      "[-------------------------------------------->     ] 18/20Harrison worked at Kensho.\n",
      "[----------------------------------------------->  ] 19/20{'a': 1, 'b': 2, 'c': 3}\n",
      "[------------------------------------------------->] 20/20"
     ]
    }
   ],
   "source": [
    "# Run eval on base chain\n",
    "run_id = uuid.uuid4().hex[:4]\n",
    "project_name = \"context-stuffing-no-langgraph\"\n",
    "results = client.run_on_dataset(\n",
    "    dataset_name=\"lcel-teacher-eval\",\n",
    "    llm_or_chain_factory=lambda: (lambda x: x[\"question\"]) | chain_base_case,\n",
    "    evaluation=evaluation_config,\n",
    "    project_name=f\"{run_id}-{project_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '57ac-context-stuffing-with-langgraph' at:\n",
      "https://smith.langchain.com/o/251ecdd9-2ca8-5dd9-ab04-c02e1fdaa47b/datasets/35eb6da5-2be1-4d79-b992-c8c8032be8ce/compare?selectedSessions=bdb57f76-effd-4fe9-bd15-0da4aefb4c65\n",
      "\n",
      "View all tests for Dataset lcel-teacher-eval at:\n",
      "https://smith.langchain.com/o/251ecdd9-2ca8-5dd9-ab04-c02e1fdaa47b/datasets/35eb6da5-2be1-4d79-b992-c8c8032be8ce\n",
      "[>                                                 ] 0/20---GENERATE SOLUTION---\n",
      "---GENERATE SOLUTION---\n",
      "---GENERATE SOLUTION---\n",
      "---GENERATE SOLUTION---\n",
      "---GENERATE SOLUTION---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "{'joke': \"Why don't bears wear shoes?\\nBecause they have bear feet!\", 'poem': \"In the forest deep and wild,\\nWhere shadows dance and whispers mild,\\nThe mighty bears roam free and strong,\\nTheir fur a coat both thick and long.\\n\\nWith paws that tread the earth with grace,\\nThey move with power, find their place,\\nIn nature's grand and ancient dance,\\nA symbol of strength and chance.\\n\\nBut do not fear the bear's fierce might,\\nFor in their hearts burns gentle light,\\nThey care for cubs with tender care,\\nIn dens of safety, warm and rare.\\n\\nSo let us marvel at their might,\\nAnd honor them in day and night,\\nFor bears are creatures pure and true,\\nIn forests green and skies of blue.\"}\n",
      "---CODE BLOCK CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "{'joke': 'Why did the bear bring a flashlight to the party? \\n\\nBecause he heard it was going to be a \"beary\" good time!', 'poem': 'In the forest deep and wild,\\nWhere the trees stand tall and proud,\\nThere roams a creature, strong and mild,\\nThe bear, so noble and unbowed.\\n\\nWith fur as dark as night,\\nAnd eyes that gleam with might,\\nHe moves with grace and power,\\nIn his domain, he is the tower.\\n\\nA symbol of strength and might,\\nYet gentle with his cubs in sight,\\nThe bear is a majestic sight to see,\\nIn the heart of the forest, wild and free.\\n\\nSo let us cherish and protect,\\nThese creatures so grand and perfect,\\nFor in their presence, we can find,\\nA reminder of the beauty of nature, intertwined.'}\n",
      "[->                                                ] 1/20---GENERATE SOLUTION---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "content=\"Why did the bear break up with his girlfriend?\\nBecause he couldn't bear the relationship anymore!\" response_metadata={'finish_reason': 'stop', 'logprobs': None}\n",
      "---CODE BLOCK CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "content='Why did the bear bring a flashlight to the party? \\n\\nBecause he heard it was going to be a \"beary\" good time!' response_metadata={'finish_reason': 'stop', 'logprobs': None}\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "[---->                                             ] 2/20---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATE SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "content='Hello Bob! How can I assist you today?' response_metadata={'finish_reason': 'stop', 'logprobs': None}\n",
      "{'history': [HumanMessage(content='hi im bob'), AIMessage(content='Hello Bob! How can I assist you today?')]}\n",
      "---CODE BLOCK CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "content='Hello Bob! How can I assist you today?' response_metadata={'finish_reason': 'stop', 'logprobs': None}\n",
      "{'history': [HumanMessage(content='hi im bob'), AIMessage(content='Hello Bob! How can I assist you today?')]}\n",
      "[------->                                          ] 3/20---GENERATE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 9fed0144-cd6b-4c61-918e-b88fd226167a with inputs {'question': 'How can I use a custom function to route between 2 chains in LCEL?'}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 296682, Requested 74889. Please try again in 14.314s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------->                                        ] 4/20---GENERATE SOLUTION---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "Python is a versatile and powerful programming language that is known for its simplicity and readability. It was created by Guido van Rossum in the late 1980s and has since become one of the most popular programming languages in the world. One interesting fact about Python is that it is named after the British comedy show \"Monty Python's Flying Circus,\" which Guido van Rossum was a fan of. Additionally, Python has a large and active community of developers who contribute to its open-source ecosystem, making it a great choice for beginners and experienced programmers alike.\n",
      "---CODE BLOCK CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 8ac4ac90-876a-4034-8ae0-6f5b77c5d8e7 with inputs {'question': 'How can I make the output of my LCEL chain a string?'}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 290110, Requested 74885. Please try again in 12.999s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----------->                                      ] 5/20---GENERATE SOLUTION---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "Python programming language was created by Guido van Rossum and released in 1991. The name \"Python\" was inspired by the British comedy show \"Monty Python's Flying Circus.\" Python is known for its simplicity and readability, making it a popular choice for beginners and experienced programmers alike. Python has a wide range of applications, including web development, data analysis, artificial intelligence, and automation. It has a large and active community of developers who contribute to its libraries, frameworks, and tools. Python is also the fastest-growing programming language according to the TIOBE Index, showing its increasing popularity and relevance in the tech industry.\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "[-------------->                                   ] 6/20---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---GENERATE SOLUTION---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 37b94cd3-d58a-4b9b-bbc1-09d3d8f8fec9 with inputs {'question': \"My LCEL map contains the key 'question'. What is the difference between using itemgetter('question'), lambda x: x['question'], and x.get('question')?\"}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 291271, Requested 74909. Please try again in 13.236s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----------------->                                ] 7/20"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 4ca5aaa8-62cd-4e3a-bf3b-bd5b1c720463 with inputs {'question': 'How do I set up a retrieval-augmented generation chain using LCEL that accepts a string as input?'}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 287726, Requested 75534. Please try again in 12.652s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------->                              ] 8/20---GENERATE SOLUTION---\n",
      "---GENERATE SOLUTION---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 164f5ddd-2b11-404d-9bb6-025f70d8acab with inputs {'question': 'How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?'}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 292196, Requested 75505. Please try again in 13.54s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------->                            ] 9/20---GENERATE SOLUTION---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example e7cf0e3f-08a1-4c7c-8bb0-142ea90a7e69 with inputs {'question': 'How can we apply a function call to an LLM in an LCEL chain?'}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 290653, Requested 74887. Please try again in 13.108s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Chain failed for example bb3c615b-4fd2-4f62-84ab-ec0da8982a05 with inputs {'question': \"I have a LCEL runnable, chain, and am passing in a map w/ {'question' 'where did harrison work', 'language': 'italian'}. How can I extract the value of 'language' to pass to my prompt?\"}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 290684, Requested 74918. Please try again in 13.12s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------->                      ] 11/20---GENERATE SOLUTION---\n",
      "---GENERATE SOLUTION---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "[----------------------------->                    ] 12/20"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 577aa8c7-cbe7-49fe-bfa2-c3ceb48fbb68 with inputs {'question': \"I'm invoking a LCEL chain with a map that contain {'question': 'how do I use Anthropic?'}. The full chain definition is full_chain = {'question': lambda x: x['question']} | sub_chain. Why is a lambda used?\"}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 285861, Requested 74923. Please try again in 12.156s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------->                  ] 13/20---GENERATE SOLUTION---\n",
      "---GENERATE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example ceefc5fe-b782-46a7-9bf5-d95d0d7fcd40 with inputs {'question': \"I am passing a map with {'num': 1} to a LCEL chain. How can I add an extra key num2 to this map that adds 1 to the value of num. Then I want to assign this new map to a new key named output?\"}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 296231, Requested 74920. Please try again in 14.23s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Chain failed for example c05120e1-7f1b-40d4-9a94-dd52d7610ee5 with inputs {'question': \"I am passing text key 'foo' to my prompt and want to process it with a function, process_text(...), prior to the prompt. How can I do this using LCEL?\"}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 295620, Requested 74910. Please try again in 14.106s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------->            ] 15/20---GENERATE SOLUTION---\n",
      "---GENERATE SOLUTION---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 0d903a3e-91bc-4131-9d48-789b48eadf17 with inputs {'question': 'How can I configure the temperature of an LLM when invoking the LCEL chain?'}\n",
      "Error Type: RateLimitError, Message: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-kv3VikNMB4MVTBLwKd3vFvUk on tokens per min (TPM): Limit 300000, Used 296151, Requested 74891. Please try again in 14.208s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------->          ] 16/20---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "[----------------------------------------->        ] 17/20---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "{'a': 1, 'b': 2, 'c': 3}\n",
      "---CODE BLOCK CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "{'a': 1, 'b': 2, 'c': 3}\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "[-------------------------------------------->     ] 18/20---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---RE-GENERATE SOLUTION w/ ERROR FEEDBACK---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "Harrison worked at Kensho.\n",
      "---CODE BLOCK CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE IMPORTS---\n",
      "---CODE IMPORT CHECK: SUCCESS---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "---CHECKING CODE EXECUTION---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECIDE TO TEST CODE EXECUTION---\n",
      "---DECISION: TEST CODE EXECUTION---\n",
      "[----------------------------------------------->  ] 19/20Harrison worked at Kensho.\n",
      "[------------------------------------------------->] 20/20"
     ]
    }
   ],
   "source": [
    "### LangGraph\n",
    "\n",
    "\n",
    "def check_import(run: Run, example: Union[Example, None] = None):\n",
    "    model_outputs = run.outputs[\"keys\"]\n",
    "    imports = model_outputs[\"imports\"]\n",
    "    try:\n",
    "        exec(imports)\n",
    "        score = 1\n",
    "    except:\n",
    "        score = 0\n",
    "    return EvaluationResult(key=\"check_import\", score=score)\n",
    "\n",
    "\n",
    "def check_execution(run: Run, example: Union[Example, None] = None):\n",
    "    model_outputs = run.outputs[\"keys\"]\n",
    "    imports = model_outputs[\"imports\"]\n",
    "    code = model_outputs[\"code\"]\n",
    "    code_to_execute = imports + \"\\n\" + code\n",
    "    try:\n",
    "        exec(code_to_execute)\n",
    "        score = 1\n",
    "    except:\n",
    "        score = 0\n",
    "    return EvaluationResult(key=\"check_execution\", score=score)\n",
    "\n",
    "\n",
    "# Config\n",
    "evaluation_config = RunEvalConfig(\n",
    "    custom_evaluators=[check_import, check_execution],\n",
    ")\n",
    "\n",
    "config = {\"recursion_limit\": 50}\n",
    "\n",
    "\n",
    "def model(input: dict):\n",
    "    return app.invoke({\"keys\": {**input, \"iterations\": 0}}, config=config)\n",
    "\n",
    "\n",
    "run_id = uuid.uuid4().hex[:4]\n",
    "project_name = \"context-stuffing-with-langgraph\"\n",
    "langgraph_results = client.run_on_dataset(\n",
    "    dataset_name=\"lcel-teacher-eval\",\n",
    "    llm_or_chain_factory=model,\n",
    "    evaluation=evaluation_config,\n",
    "    project_name=f\"{run_id}-{project_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will have to update these to match the tests you ran.\n",
    "# The test name can be found at langgraph_results[\"project_name\"]\n",
    "langgraph = [\n",
    "    \"80db-context-stuffing-with-langgraph\",\n",
    "    \"060c-context-stuffing-with-langgraph\",\n",
    "    \"93cd-context-stuffing-with-langgraph\",\n",
    "    \"60ef-context-stuffing-with-langgraph\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_langgraph = [\n",
    "    \"b493-context-stuffing-no-langgraph\",\n",
    "    \"eb8a-context-stuffing-no-langgraph\",\n",
    "    \"b88c-context-stuffing-no-langgraph\",\n",
    "    \"0aaa-context-stuffing-no-langgraph\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithNotFoundError",
     "evalue": "Project 80db-context-stuffing-with-langgraph not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLangSmithNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Prepare each dataframe\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m dfs_chain1 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m     prepare_dataframe(project, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLangGraph\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, project \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(langgraph)\n\u001b[0;32m     17\u001b[0m ]\n\u001b[0;32m     18\u001b[0m dfs_chain2 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     19\u001b[0m     prepare_dataframe(project, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo LangGraph\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, project \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(no_langgraph)\n\u001b[0;32m     21\u001b[0m ]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Combine all dataframes\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[34], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Prepare each dataframe\u001b[39;00m\n\u001b[0;32m     14\u001b[0m dfs_chain1 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mprepare_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLangGraph\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, project \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(langgraph)\n\u001b[0;32m     17\u001b[0m ]\n\u001b[0;32m     18\u001b[0m dfs_chain2 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     19\u001b[0m     prepare_dataframe(project, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo LangGraph\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, project \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(no_langgraph)\n\u001b[0;32m     21\u001b[0m ]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Combine all dataframes\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[34], line 5\u001b[0m, in \u001b[0;36mprepare_dataframe\u001b[1;34m(project, trial_number, chain)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_dataframe\u001b[39m(project, trial_number, chain):\n\u001b[1;32m----> 5\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_test_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeedback.check_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeedback.check_import\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      7\u001b[0m     df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.question\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeedback.check_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeedback.check_import\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\kevol\\anaconda3\\envs\\lang\\lib\\site-packages\\langsmith\\client.py:2042\u001b[0m, in \u001b[0;36mClient.get_test_results\u001b[1;34m(self, project_id, project_name)\u001b[0m\n\u001b[0;32m   2040\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2041\u001b[0m example_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2042\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m runs:\n\u001b[0;32m   2043\u001b[0m     row \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   2044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: r\u001b[38;5;241m.\u001b[39mreference_example_id,\n\u001b[0;32m   2045\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39minputs\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m   2046\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (r\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m   2047\u001b[0m     }\n\u001b[0;32m   2048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mfeedback_stats:\n",
      "File \u001b[1;32mc:\\Users\\kevol\\anaconda3\\envs\\lang\\lib\\site-packages\\langsmith\\client.py:1552\u001b[0m, in \u001b[0;36mClient.list_runs\u001b[1;34m(self, project_id, project_name, run_type, trace_id, reference_example_id, query, filter, trace_filter, tree_filter, execution_order, parent_run_id, start_time, error, run_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(project_name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1550\u001b[0m         project_name \u001b[38;5;241m=\u001b[39m [project_name]\n\u001b[0;32m   1551\u001b[0m     project_ids\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m-> 1552\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_project(project_name\u001b[38;5;241m=\u001b[39mname)\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m project_name]\n\u001b[0;32m   1553\u001b[0m     )\n\u001b[0;32m   1555\u001b[0m body_query: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m\"\u001b[39m: project_ids \u001b[38;5;28;01mif\u001b[39;00m project_ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_type,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1572\u001b[0m }\n\u001b[0;32m   1573\u001b[0m body_query \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m body_query\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n",
      "File \u001b[1;32mc:\\Users\\kevol\\anaconda3\\envs\\lang\\lib\\site-packages\\langsmith\\client.py:1552\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1549\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(project_name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1550\u001b[0m         project_name \u001b[38;5;241m=\u001b[39m [project_name]\n\u001b[0;32m   1551\u001b[0m     project_ids\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m-> 1552\u001b[0m         [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m project_name]\n\u001b[0;32m   1553\u001b[0m     )\n\u001b[0;32m   1555\u001b[0m body_query: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m\"\u001b[39m: project_ids \u001b[38;5;28;01mif\u001b[39;00m project_ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_type,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1572\u001b[0m }\n\u001b[0;32m   1573\u001b[0m body_query \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m body_query\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n",
      "File \u001b[1;32mc:\\Users\\kevol\\anaconda3\\envs\\lang\\lib\\site-packages\\langsmith\\utils.py:92\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     invalid_group_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_groups[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m invalid_groups]\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m     )\n\u001b[1;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevol\\anaconda3\\envs\\lang\\lib\\site-packages\\langsmith\\client.py:1988\u001b[0m, in \u001b[0;36mClient.read_project\u001b[1;34m(self, project_id, project_name, include_stats)\u001b[0m\n\u001b[0;32m   1986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   1987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1988\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithNotFoundError(\n\u001b[0;32m   1989\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1990\u001b[0m         )\n\u001b[0;32m   1991\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mTracerSessionResult(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresult[\u001b[38;5;241m0\u001b[39m], _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url)\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mTracerSessionResult(\n\u001b[0;32m   1993\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson(), _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url\n\u001b[0;32m   1994\u001b[0m )\n",
      "\u001b[1;31mLangSmithNotFoundError\u001b[0m: Project 80db-context-stuffing-with-langgraph not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_dataframe(project, trial_number, chain):\n",
    "    df = client.get_test_results(project_name=project)\n",
    "    df = df.dropna(subset=[\"feedback.check_execution\", \"feedback.check_import\"])\n",
    "    df = df[[\"input.question\", \"feedback.check_execution\", \"feedback.check_import\"]]\n",
    "    df[\"trial #\"] = trial_number\n",
    "    df[\"chain\"] = chain\n",
    "    return df\n",
    "\n",
    "\n",
    "# Prepare each dataframe\n",
    "dfs_chain1 = [\n",
    "    prepare_dataframe(project, i + 1, \"LangGraph\")\n",
    "    for i, project in enumerate(langgraph)\n",
    "]\n",
    "dfs_chain2 = [\n",
    "    prepare_dataframe(project, i + 1, \"No LangGraph\")\n",
    "    for i, project in enumerate(no_langgraph)\n",
    "]\n",
    "\n",
    "# Combine all dataframes\n",
    "final_df = pd.concat(dfs_chain1 + dfs_chain2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NO MORE MONEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
